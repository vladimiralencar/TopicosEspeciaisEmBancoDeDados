{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de Mídias Sociais: Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para autenticar no twitter, temos que criar uma conta e accessar: <br />\n",
    "http://twitter.com/apps/new <br />\n",
    "Gerar as chaves de autenticação, ex:<br />\n",
    "CONSUMER_KEY = 'DcPtlOYZ07jilc3PgYJLUPdfdfdRjJXXXXX' <br />\n",
    "CONSUMER_SECRET = 'bREXzcIMNLKhRpOhQGFuEKTaJhRhHcdfdfdjD3dKBe682Yt2MOiNv6bXXXX' <br />\n",
    "OAUTH_TOKEN = '2904474861-coE0I5LEq8btCC0A18IPjqOp1G24dfdfdfvBgwXKyPi5bXXXX' <br />\n",
    "OAUTH_TOKEN_SECRET = 'ppDRYIeSXOb6J6qajqXolniwVNgbAkIEdfdfdfYDS9VfIzOboQoXXXX'<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modulo para autenticação no twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dados.txt  stopwords-portugues.txt  tokens.txt\ttokes.txt\n"
     ]
    }
   ],
   "source": [
    "!ls *.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q twitter\n",
    "# !pip install -q pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DcPtlOYZ07jilc3PgYJLUPRjJ\n",
      "bREXzcIMNLKhRpOhQGFuEKTaJhRhHcjD3dKBe682Yt2MOiNv6b\n",
      "2904474861-qPmMjcwisBQWR1cMILoXevC8QeKS3Go3PzN5lBq\n",
      "h0iaWqpk7YqsP9MAk5mU472ZOsFOtunfL76fwhEY3x9PA"
     ]
    }
   ],
   "source": [
    "!cat tokens.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura das chaves de acesso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ler_tokens_acesso():\n",
    "    filename = 'tokens.txt'\n",
    "    with open(filename) as f:\n",
    "        content = f.readlines()\n",
    "    # you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "    content = [x.strip() for x in content] \n",
    "    \n",
    "    return content[0], content[1], content[2], content[3]\n",
    "\n",
    "#CONSUMER_KEY, CONSUMER_SECRET, OAUTH_TOKEN, OAUTH_TOKEN_SECRET = ler_tokens_acesso()\n",
    "#print(CONSUMER_KEY, CONSUMER_SECRET, OAUTH_TOKEN, OAUTH_TOKEN_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<twitter.api.Twitter object at 0x7f9ae8430230>\n"
     ]
    }
   ],
   "source": [
    "import twitter\n",
    "\n",
    "def oauth_login():\n",
    "    # XXX: Go to http://twitter.com/apps/new to create an app and get values\n",
    "    # for these credentials that you'll need to provide in place of these\n",
    "    # empty string values that are defined as placeholders.\n",
    "    # See https://dev.twitter.com/docs/auth/oauth for more information \n",
    "    # on Twitter's OAuth implementation.\n",
    "    \n",
    "    #CONSUMER_KEY = 'DcPtlOYZ07jilc3PgYJLUdfdfdfJ'\n",
    "    #CONSUMER_SECRET = 'bREXzcIMNLKhRpOhQGFuEKTaJhRhHcjD3dKBe682Yt2MOdfdf'\n",
    "    \n",
    "    #OAUTH_TOKEN = '2904474861-hJHuW0Q1TGSUjOFvlgqU4ZFYvPCDO214aILxR5L'\n",
    "   # OAUTH_TOKEN_SECRET = 'aBCJrbhXVEBgZ7gKo9yCNUMjlierefMkY43rZEvXK9BFI'\n",
    "    \n",
    "    CONSUMER_KEY, CONSUMER_SECRET, OAUTH_TOKEN, OAUTH_TOKEN_SECRET = ler_tokens_acesso()\n",
    "    \n",
    "    auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                               CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    \n",
    "    twitter_api = twitter.Twitter(auth=auth)\n",
    "    return twitter_api\n",
    "\n",
    "# Sample usage\n",
    "twitter_api = oauth_login()    \n",
    "\n",
    "# Nothing to see by displaying twitter_api except that it's now a\n",
    "# defined variable\n",
    "\n",
    "print (twitter_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Módulo para buscar tweets, dado uma palavra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_search(twitter_api, q, max_results=1000, **kw):  #max_results=200\n",
    "\n",
    "    # See https://dev.twitter.com/docs/api/1.1/get/search/tweets and \n",
    "    # https://dev.twitter.com/docs/using-search for details on advanced \n",
    "    # search criteria that may be useful for keyword arguments\n",
    "    \n",
    "    # See https://dev.twitter.com/docs/api/1.1/get/search/tweets    \n",
    "    search_results = twitter_api.search.tweets(q=q, count=1000, **kw)   # count=100\n",
    "    \n",
    "    statuses = search_results['statuses']\n",
    "    \n",
    "    # Iterate through batches of results by following the cursor until we\n",
    "    # reach the desired number of results, keeping in mind that OAuth users\n",
    "    # can \"only\" make 180 search queries per 15-minute interval. See\n",
    "    # https://dev.twitter.com/docs/rate-limiting/1.1/limits\n",
    "    # for details. A reasonable number of results is ~1000, although\n",
    "    # that number of results may not exist for all queries.\n",
    "    \n",
    "    # Enforce a reasonable limit\n",
    "    max_results = min(1000, max_results)\n",
    "    \n",
    "    for _ in range(10): # 10*100 = 1000\n",
    "        try:\n",
    "            next_results = search_results['search_metadata']['next_results']\n",
    "        except KeyError: # No more results when next_results doesn't exist\n",
    "            break\n",
    "            \n",
    "        # Create a dictionary from next_results, which has the following form:\n",
    "        # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
    "        kwargs = dict([ kv.split('=') \n",
    "                        for kv in next_results[1:].split(\"&\") ])\n",
    "        \n",
    "        search_results = twitter_api.search.tweets(**kwargs)\n",
    "        statuses += search_results['statuses']\n",
    "        \n",
    "        if len(statuses) > max_results: \n",
    "            break\n",
    "            \n",
    "    return statuses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando/Recuperando no MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pymongo # pip install pymongo\n",
    "\n",
    "\n",
    "def save_to_mongo(data, mongo_db, mongo_db_coll, **mongo_conn_kw):\n",
    "    \n",
    "    # Connects to the MongoDB server running on \n",
    "    # localhost:27017 by default\n",
    "    \n",
    "    client = pymongo.MongoClient(**mongo_conn_kw)\n",
    "    \n",
    "    # Get a reference to a particular database\n",
    "    \n",
    "    db = client[mongo_db]\n",
    "    \n",
    "    # Reference a particular collection in the database\n",
    "    \n",
    "    coll = db[mongo_db_coll]\n",
    "    \n",
    "    # Perform a bulk insert and  return the IDs\n",
    "    \n",
    "    return coll.insert_many(data)\n",
    "\n",
    "def load_from_mongo(mongo_db, mongo_db_coll, return_cursor=False,\n",
    "                    criteria=None, projection=None, **mongo_conn_kw):\n",
    "    \n",
    "    # Optionally, use criteria and projection to limit the data that is \n",
    "    # returned as documented in \n",
    "    # http://docs.mongodb.org/manual/reference/method/db.collection.find/\n",
    "    \n",
    "    # Consider leveraging MongoDB's aggregations framework for more \n",
    "    # sophisticated queries.\n",
    "    \n",
    "    client = pymongo.MongoClient(**mongo_conn_kw)\n",
    "    db = client[mongo_db]\n",
    "    coll = db[mongo_db_coll]\n",
    "    \n",
    "    if criteria is None:\n",
    "        criteria = {}\n",
    "    \n",
    "    if projection is None:\n",
    "        cursor = coll.find(criteria)\n",
    "    else:\n",
    "        cursor = coll.find(criteria, projection)\n",
    "\n",
    "    # Returning a cursor is recommended for large amounts of data\n",
    "    \n",
    "    if return_cursor:\n",
    "        return cursor\n",
    "    else:\n",
    "        return [ item for item in cursor ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capturando tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TwitterHTTPError",
     "evalue": "Twitter sent status 403 for URL: 1.1/search/tweets.json using parameters: (count=1000&oauth_consumer_key=DcPtlOYZ07jilc3PgYJLUPRjJ&oauth_nonce=1611636610761326065&oauth_signature_method=HMAC-SHA1&oauth_timestamp=1729174398&oauth_token=2904474861-qPmMjcwisBQWR1cMILoXevC8QeKS3Go3PzN5lBq&oauth_version=1.0&q=ucrania&oauth_signature=0iaTUG3HuO0qZsaD918MHMV%2BJ6M%3D)\ndetails: error code: 1010",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/twitter/api.py:386\u001b[0m, in \u001b[0;36mTwitterCall._handle_response\u001b[0;34m(self, req, uri, arg_data, _timeout)\u001b[0m\n\u001b[1;32m    385\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m context\n\u001b[0;32m--> 386\u001b[0m handle \u001b[38;5;241m=\u001b[39m urllib_request\u001b[38;5;241m.\u001b[39murlopen(req, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle\u001b[38;5;241m.\u001b[39mheaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage/jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage/png\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/urllib/request.py:215\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/urllib/request.py:521\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    520\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 521\u001b[0m     response \u001b[38;5;241m=\u001b[39m meth(req, response)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/urllib/request.py:630\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 630\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39merror(\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m, request, response, code, msg, hdrs)\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/urllib/request.py:559\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    558\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/urllib/request.py:492\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    491\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 492\u001b[0m result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/urllib/request.py:639\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTwitterHTTPError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:6\u001b[0m\n",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m, in \u001b[0;36mtwitter_search\u001b[0;34m(twitter_api, q, max_results, **kw)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtwitter_search\u001b[39m(twitter_api, q, max_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):  \u001b[38;5;66;03m#max_results=200\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# See https://dev.twitter.com/docs/api/1.1/get/search/tweets and \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# See https://dev.twitter.com/docs/api/1.1/get/search/tweets    \u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     search_results \u001b[38;5;241m=\u001b[39m twitter_api\u001b[38;5;241m.\u001b[39msearch\u001b[38;5;241m.\u001b[39mtweets(q\u001b[38;5;241m=\u001b[39mq, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)   \u001b[38;5;66;03m# count=100\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     statuses \u001b[38;5;241m=\u001b[39m search_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatuses\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Iterate through batches of results by following the cursor until we\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# reach the desired number of results, keeping in mind that OAuth users\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# can \"only\" make 180 search queries per 15-minute interval. See\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Enforce a reasonable limit\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/twitter/api.py:371\u001b[0m, in \u001b[0;36mTwitterCall.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_response_with_retry(req, uri, arg_data, _timeout)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_response(req, uri, arg_data, _timeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/twitter/api.py:417\u001b[0m, in \u001b[0;36mTwitterCall._handle_response\u001b[0;34m(self, req, uri, arg_data, _timeout)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TwitterHTTPError(e, uri, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat, arg_data)\n",
      "\u001b[0;31mTwitterHTTPError\u001b[0m: Twitter sent status 403 for URL: 1.1/search/tweets.json using parameters: (count=1000&oauth_consumer_key=DcPtlOYZ07jilc3PgYJLUPRjJ&oauth_nonce=1611636610761326065&oauth_signature_method=HMAC-SHA1&oauth_timestamp=1729174398&oauth_token=2904474861-qPmMjcwisBQWR1cMILoXevC8QeKS3Go3PzN5lBq&oauth_version=1.0&q=ucrania&oauth_signature=0iaTUG3HuO0qZsaD918MHMV%2BJ6M%3D)\ndetails: error code: 1010"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "q = ''\n",
    "q = ''\n",
    "q = 'ucrania'\n",
    "q = 'furacao ian'\n",
    "q = 'ucrania'\n",
    "result = twitter_search(twitter_api, q, max_results=300)\n",
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result[\u001b[38;5;241m4\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "result[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# a Python object (dict):\n",
    "x = {\n",
    "  \"name\": \"John\",\n",
    "  \"age\": 30,\n",
    "  \"city\": \"New York\"\n",
    "}\n",
    "\n",
    "# convert into JSON:\n",
    "y = json.dumps(x)\n",
    "\n",
    "# the result is a JSON string:\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_json = json.dumps(result)\n",
    "str_result =   ''.join(result) \n",
    "result_json = json.loads(result)\n",
    "type(result_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = twitter_search(twitter_api, q, max_results=1000)\n",
    "save_to_mongo(result, 'tweets2022', q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "q = 'omicron'\n",
    "q = 'ucrania'\n",
    "twitter_api = oauth_login()\n",
    "results = []\n",
    "for i in range(2):\n",
    "    result = twitter_search(twitter_api, q, max_results=1000)\n",
    "    print ('capturando ', len(result),' tweets...')\n",
    "    results.extend(result) # junçao de 2 lstas\n",
    "print(\"Numero de tweets capturados: \", len(results))\n",
    "\n",
    "results = twitter_search(twitter_api, q, max_results=1000)\n",
    "save_to_mongo(results, 'tweets', q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[2]['created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[2]['metadata']['iso_language_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imprimindo tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = results\n",
    "i = 0\n",
    "for tweet in result:\n",
    "    #print(tweet['place'], tweet['created_at'], ' >> ', tweet['text'] , \" >> \", )\n",
    "    print(tweet['text'])\n",
    "    i += 1\n",
    "    if i > 10:\n",
    "          break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuvem de Palavras - tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalando o módulo de stopwords\n",
    "!pip install -q stop-words\n",
    "results = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "\n",
    "stop_words_portugues = get_stop_words('pt')\n",
    "print(stop_words_portugues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "stop_words_english = get_stop_words('en')\n",
    "\n",
    "#stop_words_portugues\n",
    "stop_words_portugues = get_stop_words('pt')\n",
    "\n",
    "\n",
    "#stop_words\n",
    "stop_words = stop_words_english + stop_words_portugues\n",
    "\n",
    "print(stop_words[:10])\n",
    "print(stop_words[-10:])\n",
    "print(len(stop_words_english))\n",
    "print(len(stop_words_portugues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuvem de palavras\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import re\n",
    "\n",
    "stext = \"\"\n",
    "for tweet in result:\n",
    "    if 'text' in tweet:\n",
    "        stext = stext + tweet['text'] + ' '\n",
    "        \n",
    "stext = stext.lower() # colocar para minúsculas\n",
    "\n",
    "# retirando pontuações e outros caracteres não relevantes\n",
    "stext = \"\".join([i for i in stext if ord(i)<128])\n",
    "stext = stext.replace(\"http://\",\"\")\n",
    "stext = stext.replace(\"https://\",\"\")\n",
    "stext = stext.replace(\"https\",\"\")\n",
    "stext = stext.replace(\",\",\"\")\n",
    "stext = stext.replace(\".\",\"\")\n",
    "stext = stext.replace(\"?\",\"\")\n",
    "stext = stext.replace(\"!\",\"\")\n",
    "stext = stext.replace(\"#\",\"\")\n",
    "stext = stext.replace(\"@\",\"\")\n",
    "stext = stext.replace(\"'\",\"\")\n",
    "stext = stext.replace(\"-\",\"\")\n",
    "stext = stext.replace(\"3\",\"\")\n",
    "stext = stext.replace(\"&amp;\",\"\")\n",
    "stext = stext.replace(\"|\",\"\")\n",
    "stext = stext.replace(\"tco\",\"\")\n",
    "stext = stext.replace(\"rt\",\"\")\n",
    "\n",
    "\n",
    "\n",
    "# retirando as stopwords dos tweets\n",
    "for word in stop_words:\n",
    "    my_regex = r\"\\b(?=\\w)\" + re.escape(word) + r\"\\b(?!\\w)\"\n",
    "    stext = re.sub(my_regex,\"\" ,stext)\n",
    "\n",
    "print(len(stext))\n",
    "\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud().generate(stext)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# lower max_font_size\n",
    "wordcloud = WordCloud(max_font_size=40).generate(stext)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estatistica de contagem de palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = stext.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter(words)\n",
    "#print (c.most_common()[:30])\n",
    "most_common = c.most_common()[:40]\n",
    "\n",
    "for i in most_common:\n",
    "    print (i[0])\n",
    "#for item in words:  #[words, screen_names, hashtags]:\n",
    "#    c = Counter(item)\n",
    "#    print (c.most_common()[:20]) # top 10\n",
    "#    print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    print (c.popitem())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## colocando as palavras e suas frequencias do twitter num dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(([word, freq] for word, freq in most_common), columns=['Word', 'Frequency'])\n",
    "df[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "fig.add_subplot(1,1,1)\n",
    "\n",
    "df2 = df[:25]\n",
    "\n",
    "ax = sns.barplot(y='Word', x = 'Frequency', data=df2, palette=\"Set3\" )\n",
    "#sns.despine()\n",
    "ax.set(xlabel='Tweets words frequency', ylabel='Word')\n",
    "#ax.set(xticks=range(1, 9), xticklabels=list([10,20,30,40,50,60,70,80,90]))\n",
    "\n",
    "#ax.set(xticklabels=list(range(0,90,10)))\n",
    "#plt.figure(figsize=(20,10))\n",
    "\n",
    "ax.axes.set_title(\"Tweets words frequency\",fontsize=40)\n",
    "ax.set_xlabel(\"Frequency\",fontsize=26)\n",
    "ax.set_ylabel(\"Word\",fontsize=20)\n",
    "ax.tick_params(labelsize=15)\n",
    "#sns.plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## achar os países de onde foram gerados os tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place = []\n",
    "results = result.copy()\n",
    "for result in results:\n",
    "    if 'place'in result and result['place']:\n",
    "        place.append(result['place']['country'])\n",
    "place = set(place)\n",
    "print(len(place) )\n",
    "print(place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cidades\n",
    "place = []\n",
    "for result in results:\n",
    "    if 'user'  in result:\n",
    "        place.append(result['user']['location'])\n",
    "location = set(place)\n",
    "len(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    print(location.pop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## contar a ocorrencia de cada cidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter(location)\n",
    "#print(c)\n",
    "cidades = c.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## colocando as cidades, frequencia num dataframe e salvando no excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(([cidade, freq] for cidade, freq in cidades), columns=['city', 'frequency'])\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('cidades.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Convert the dataframe to an XlsxWriter Excel object.\n",
    "df.to_excel(writer, sheet_name='Sheet1')\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()\n",
    "df[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "fig.add_subplot(1,1,1)\n",
    "\n",
    "df2 = df[:25]\n",
    "\n",
    "ax = sns.barplot(y='city', x = 'frequency', data=df2, palette=\"Set3\" )\n",
    "ax.set(xlabel='Tweets cities frequency', ylabel='cities')\n",
    "ax.axes.set_title(\"Tweets cities frequency\",fontsize=40)\n",
    "ax.set_xlabel(\"frequency\",fontsize=26)\n",
    "ax.set_ylabel(\"cities\",fontsize=20)\n",
    "ax.tick_params(labelsize=15)\n",
    "#sns.plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capturar tweets com coodenadas de lat, long para cidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def captura_tweets_com_coordenadas(tweets):\n",
    "    tweets_com_coordenadas = []\n",
    "    for tweet in tweets:\n",
    "        if tweet['place'] is not None:\n",
    "            tweets_com_coordenadas.append(tweet)\n",
    "    return (tweets_com_coordenadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_com_coordenadas = captura_tweets_com_coordenadas(results)\n",
    "print(len(tweets_com_coordenadas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_com_coordenadas)/len(results) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_com_coordenadas[0]['place']['bounding_box']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_com_coordenadas[0]['place']['bounding_box']['coordinates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_com_coordenadas[0]['place']['bounding_box']['coordinates'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def captura_coordenadas(tweets):\n",
    "    coordenadas = []\n",
    "    for tweet in tweets:\n",
    "        coord = tweet['place']['bounding_box']['coordinates'][0][0]\n",
    "        coordenadas.append(coord)\n",
    "    return coordenadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_coordenadasXY = captura_coordenadas(tweets_com_coordenadas)\n",
    "tweets_coordenadasXY[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_com_coordenadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitudes_tweets =  [ y for x,y in tweets_coordenadasXY]\n",
    "longitudes_tweets =  [ x for x,y in tweets_coordenadasXY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordenadas_distintas = set(latitudes_tweets)\n",
    "len(coordenadas_distintas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(latitudes_tweets[:15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(longitudes_tweets[:15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df10 = pd.DataFrame({'latitude': latitudes_tweets, 'longitude':longitudes_tweets })\n",
    "df10.head()                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11 = df10.drop_duplicates()\n",
    "df11 = df11.reset_index(drop=True)\n",
    "len(df11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11.to_csv('coordenadas-tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df11 = pd.read_csv('coordenadas-tweets.csv')\n",
    "df11.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df11)):\n",
    "    #print(i)\n",
    "    row = df11.loc[i]\n",
    "    #print(row)\n",
    "    print('latitude:', row['latitude'], ', longitude:', row['longitude'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(latitudes_tweets[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(longitudes_tweets[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ler coordenadas de um arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 = pd.read_csv('coordenadas-tweets-Novembro-2019.csv')\n",
    "df10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12 = df10.copy()\n",
    "df12.head()\n",
    "df12 = df12.dropna()\n",
    "len(df12)\n",
    "df12.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remover tuplas duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df13 = df12.drop_duplicates()\n",
    "df13 = df13.reset_index(drop=True)\n",
    "len(df13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Georeferenciar os tweets no mapa mundi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_coordenadasXY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.utilities import write_png\n",
    "\n",
    "mapit = folium.Map(location=[30, 0], zoom_start=2)\n",
    "for coord in tweets_coordenadasXY:\n",
    "    folium.CircleMarker( location=[ coord[1], coord[0] ], radius=2).add_to( mapit ) \n",
    "                    # color='#0080bb', fill_color='#0080bb'\n",
    "\n",
    "\n",
    "folium.Map.save(mapit, \"index.html\")\n",
    "\n",
    "\n",
    "mapit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# twitter trends - assuntos mais comentados do momento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import twitter\n",
    "\n",
    "# Acessar os tends da localidade\n",
    "#http://woeid.rosselliot.co.nz/\n",
    "\n",
    "\n",
    "def twitter_trends(twitter_api, woe_id):\n",
    "    # Prefix ID with the underscore for query string parameterization.\n",
    "    # Without the underscore, the twitter package appends the ID value\n",
    "    # to the URL itself as a special-case keyword argument.\n",
    "    return twitter_api.trends.place(_id=woe_id)\n",
    "\n",
    "# Sample usage\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "\n",
    "# See https://dev.twitter.com/docs/api/1.1/get/trends/place and\n",
    "# http://developer.yahoo.com/geo/geoplanet/ for details on\n",
    "# Yahoo! Where On Earth ID\n",
    "\n",
    "WORLD_WOE_ID = 1\n",
    "world_trends = twitter_trends(twitter_api, WORLD_WOE_ID)\n",
    "#print json.dumps(world_trends, indent=1)\n",
    "\n",
    "US_WOE_ID = 23424977\n",
    "us_trends = twitter_trends(twitter_api, US_WOE_ID)\n",
    "#print (json.dumps(us_trends, indent=1))\n",
    "#us_trends[0]\n",
    "\n",
    "#for trend in us_trends:\n",
    "#    print (trend['as_of']['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_trends(country_trends):\n",
    "    trends = country_trends[0]['trends']\n",
    "    for trend in trends:\n",
    "        print (trend['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "WORLD_WOE_ID = 1\n",
    "world_trends = twitter_trends(twitter_api, WORLD_WOE_ID)\n",
    "print (\"topicos mais comentados: Mundo\", datetime.now())\n",
    "find_trends(world_trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_WOE_ID = 23424977\n",
    "us_trends = twitter_trends(twitter_api, US_WOE_ID)\n",
    "print (\"topicos mais comentados: USA\", datetime.now())\n",
    "find_trends(us_trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BR_WOE_ID = 23424768\n",
    "br_trends = twitter_trends(twitter_api, BR_WOE_ID)\n",
    "print (\"topicos mais comentados: Brasil\", datetime.now())\n",
    "find_trends(br_trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TM_WOE_ID = 32566\n",
    "tm_trends = twitter_trends(twitter_api, TM_WOE_ID)\n",
    "print (\"topicos mais comentados: UK\")\n",
    "find_trends(tm_trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TM_WOE_ID = 455827\n",
    "tm_trends = twitter_trends(twitter_api, TM_WOE_ID)\n",
    "print (\"topicos mais comentados: São Paulo\")\n",
    "find_trends(tm_trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de Sentimentos - Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textblob\n",
    "#!python -m pip install --pre googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install googletrans\n",
    "#!pip install google_trans_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "\n",
    "#translator.translate('veritas lux mea', src='la')\n",
    "translator.translate('oi, tudo bem', src='pt').text #, src='pt', dest='en') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando as polaridades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob as tb\n",
    "txt = 'bad girl in NY'\n",
    "analysis = tb(txt)\n",
    "analysis.sentiment.polarity\n",
    "analysis.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'neutral'\n",
    "analysis = tb(txt)\n",
    "analysis.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'good'\n",
    "analysis = tb(txt)\n",
    "analysis.sentiment.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# polaridade de tweets em ingles e portugues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob as tb #NLP module\n",
    "import numpy as np #for some calculation. You may want to use your favorite module for this, such as statistics\n",
    "\n",
    "# polaridade de tweets em ingles e portugues\n",
    "def polarity_tweets_pais(query):\n",
    "    from googletrans import Translator\n",
    "    \n",
    "    translator = Translator() # google tradutor\n",
    "\n",
    "    twitter_api = oauth_login()\n",
    "    numero_iteracoes = 1\n",
    "    results = []\n",
    "    for i in range(numero_iteracoes):\n",
    "        max_results = 500 # 1000\n",
    "        result = twitter_search(twitter_api, query, max_results=max_results)\n",
    "        print ('capturando ', len(result),' tweets...')\n",
    "        #results.append(result)\n",
    "        results += result\n",
    "\n",
    "    analysis = None\n",
    "    pos_polarity = 0\n",
    "    neutral_polarity = 0\n",
    "    neg_polarity = 0\n",
    "    sum_polarity = 0\n",
    "    print(\"calculando polaridades...\")\n",
    "\n",
    "    for tweet in results:\n",
    "        text_tweet = tweet['text']\n",
    "        \n",
    "        # retirando caracteres malucos\n",
    "        text_tweet = \"\".join([i for i in text_tweet if ord(i)<128])\n",
    "        \n",
    "        frase = tb(text_tweet)       \n",
    "        language = tweet['lang'] #frase.detect_language()\n",
    "        if language == 'br' or language == 'pt' :\n",
    "            text_en = translator.translate(text_tweet, src='pt').text  \n",
    "            analysis = tb(text_en)\n",
    "        elif language == 'en':   \n",
    "            analysis = frase\n",
    "        else:\n",
    "            analysis = \"\"\n",
    "            \n",
    "        if analysis != \"\": \n",
    "            if (analysis.sentiment.polarity > 0):\n",
    "                pos_polarity += 1\n",
    "            elif (analysis.sentiment.polarity <= 0):\n",
    "                neg_polarity += 1\n",
    "            sum_polarity += analysis.sentiment.polarity\n",
    "\n",
    "    return sum_polarity, pos_polarity, neg_polarity, neutral_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'tesla'\n",
    "\n",
    "\"\"\"\n",
    "mean_polarity, pos_polarity, neg_polarity, neutral_polarity = polarity_tweets_pais(q)\n",
    "total = pos_polarity + neg_polarity + neutral_polarity \n",
    "print('SENTIMENT AVERAGE: ' , mean_polarity/total)\n",
    "print('Positive: ', pos_polarity, (pos_polarity/total) * 100)\n",
    "print('Negative: ', neg_polarity, (neg_polarity/total) * 100)\n",
    "print('neutral: ', neutral_polarity, (neutral_polarity/total) * 100\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=['name', 'pos', 'negative', 'sentiment_avg'])\n",
    "queries = [ 'tesla', 'facebook', 'apple', 'xiaomi']\n",
    "\n",
    "for q in queries:\n",
    "    mean_polarity, pos_polarity, neg_polarity, neutral_polarity = polarity_tweets_pais(q)\n",
    "    total = pos_polarity + neg_polarity + neutral_polarity \n",
    "    print('Query: ', q)\n",
    "    print('SENTIMENT AVERAGE: ' , mean_polarity/total)\n",
    "    print('Positive: ', pos_polarity, (pos_polarity/total) * 100)\n",
    "    print('Negative: ', neg_polarity, (neg_polarity/total) * 100)\n",
    "    #print('neutral: ', neutral_polarity, (neutral_polarity/total) * 100)\n",
    "    print('----------------------------------------------')\n",
    "    \n",
    "    i = len(df)\n",
    "    df.loc[i] = [q, pos_polarity, neg_polarity, mean_polarity/total]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"%pos\"] = df.pos / (df.pos + df.negative)\n",
    "df[\"%neg\"] = df.negative / (df.pos + df.negative)\n",
    "df[\"%pos\"] = df[\"%pos\"].apply(lambda x: int(round(x,1) * 100))\n",
    "df[\"%neg\"] = df[\"%neg\"].apply(lambda x: int(round(x,1) * 100))\n",
    "df.loc[:, ['name', \"%pos\", \"%neg\", 'sentiment_avg' ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gravando os dados do dataframe num arquivo .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "data_agora = str(datetime.now())\n",
    "df.to_csv(\"sentimentos-\"+ data_agora +\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, ['name', \"%pos\", \"%neg\" ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
